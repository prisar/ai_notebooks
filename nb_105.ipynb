{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBRhsFtxwo8hB2bkfBMix4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisar/ai_notebooks/blob/main/nb_105.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LqMaGHNG9pxU"
      },
      "outputs": [],
      "source": [
        "# Authentication and service account setup\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import os\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set project ID\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'mrc-quant-ml'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q google-genai google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "iYjaKf44917E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize\n",
        "from google.genai import Client\n",
        "from google.genai.types import Part, VideoMetadata, FileData\n",
        "from google.cloud import storage\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import nest_asyncio\n",
        "import time # Import time module for delays\n",
        "import moviepy.editor as mp # Import moviepy for video duration\n",
        "\n",
        "\n",
        "def summarize_video_chunk(video_uri: str, start_offset: str, end_offset: str, prompt: str = \"Analyze this video and provide a summary.\"):\n",
        "    \"\"\"Summarizes a video chunk using the Gemini API.\"\"\"\n",
        "    client = Client(\n",
        "        vertexai=True,\n",
        "        project=\"mrc-quant-ml\",\n",
        "        location=\"us-central1\",\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        contents=[\n",
        "            Part(\n",
        "                video_metadata=VideoMetadata(\n",
        "                    fps=1,\n",
        "                    start_offset=start_offset,\n",
        "                    end_offset=end_offset\n",
        "                ),\n",
        "                file_data=FileData(\n",
        "                    file_uri=video_uri,\n",
        "                    mime_type=\"video/mp4\",\n",
        "                ),\n",
        "            ),\n",
        "            prompt\n",
        "        ],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Function to get video duration\n",
        "async def get_video_duration(video_uri: str) -> int:\n",
        "    \"\"\"Gets the duration of a video from a GCS URI.\"\"\"\n",
        "    try:\n",
        "        # Assuming the video is in a GCS bucket\n",
        "        client = storage.Client()\n",
        "        bucket_name, blob_name = video_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
        "        bucket = client.get_bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        # Download the video temporarily to get duration (consider optimizing this)\n",
        "        temp_file = f\"/tmp/{blob_name.split('/')[-1]}\"\n",
        "        blob.download_to_filename(temp_file)\n",
        "        clip = mp.VideoFileClip(temp_file)\n",
        "        duration = int(clip.duration)\n",
        "        os.remove(temp_file) # Clean up the temporary file\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting video duration: {e}\")\n",
        "        return 0 # Return 0 or raise an error based on desired behavior\n",
        "\n",
        "# Batch processing optimization\n",
        "async def process_video_chunks_parallel(video_uri: str, chunk_duration_minutes: int = 30, max_workers: int = 4, delay_seconds: int = 1):\n",
        "    \"\"\"Process video chunks in parallel for better throughput with 30-minute intervals and a delay between API calls.\"\"\"\n",
        "    chunk_duration = chunk_duration_minutes * 60 # Convert minutes to seconds\n",
        "\n",
        "    total_duration = 7302 # await get_video_duration(video_uri)\n",
        "    print(f\"Total video duration: {total_duration} seconds\")\n",
        "    if total_duration == 0:\n",
        "        print(\"Could not get video duration. Aborting processing.\")\n",
        "        return []\n",
        "\n",
        "    chunks = [(i, min(i + chunk_duration, total_duration))\n",
        "              for i in range(0, total_duration, chunk_duration)]\n",
        "\n",
        "    def run_summarize_chunk(start, end):\n",
        "      \"\"\"Helper function to run the summarize_video_chunk coroutine.\"\"\"\n",
        "      return asyncio.run(process_chunk_with_delay(start, end))\n",
        "\n",
        "    async def process_chunk_with_delay(start, end):\n",
        "        \"\"\"Helper function to process a chunk with a delay.\"\"\"\n",
        "        summary = summarize_video_chunk(video_uri, f\"{start}s\", f\"{end}s\")\n",
        "        await asyncio.sleep(delay_seconds) # Add delay between calls\n",
        "        return summary\n",
        "\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        tasks = [\n",
        "            loop.run_in_executor(\n",
        "                executor,\n",
        "                run_summarize_chunk,\n",
        "                start,\n",
        "                end\n",
        "            ) for start, end in chunks\n",
        "        ]\n",
        "\n",
        "        summaries = await asyncio.gather(*tasks)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Example usage with error handling\n",
        "video_uri = \"gs://mrc-quant-ml-video-analysis/videoplayback.mp4\"\n",
        "\n",
        "# Example of how to use the parallel processing function\n",
        "\n",
        "nest_asyncio.apply() # Apply this if running in Colab\n",
        "\n",
        "try:\n",
        "    # Add delay_seconds parameter to control delay\n",
        "    all_summaries = asyncio.run(process_video_chunks_parallel(video_uri, chunk_duration_minutes=30, delay_seconds=5))\n",
        "    for i, summary in enumerate(all_summaries):\n",
        "        print(f\"Summary for chunk {i+1}:\\n{summary}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during parallel processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYOfIGJ-957g",
        "outputId": "3ec89cd1-aca5-4374-a909-048ac60717e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total video duration: 7302 seconds\n",
            "Summary for chunk 1:\n",
            "Here is a summary of the video:\n",
            "\n",
            "The speaker is streaming, talking about AI. The stream is titled \"Diffusion Beats Autoregressive.\"  He starts by saying that the AI generated the thumbnails for the video using Open AI's GPT, but hilariously misspelled 'autoregressive'.\n",
            "\n",
            "He speaks about two AI model types and presents a graph of scaling laws. He then says that you would expect a language model that has 24 trillion parameters, and has been trained on 100 trillion tokens with high variability to be smart.\n",
            "\n",
            "He says he almost cringed at something he heard someone say on a podcast with British sounding accent, who said something so simple it didn't seem smart.\n",
            "\n",
            "He proceeds to define what is over fitting vs under fitting by giving examples.  For Machine Learning models, he says over fitting means \"fitting more closely to the data, but not to a realistic standard.\"\n",
            "\n",
            "He finishes by saying what he would determine model undershooting, overfitting.\n",
            "\n",
            "Summary for chunk 2:\n",
            "Here is a summary of the video:\n",
            "\n",
            "The video depicts a person going over scientific papers on the screen. In one instance, the paper is titled “Training Curve Envelope” which presents a range of model sizes that include 70 million to 10 billion. It has points to estimate the optimal model size for a given compute budget and the optimal number of training tokens to train each model.\n",
            "\n",
            "Later, the video depicts a man looking at and examining a paper titled, “Distillation Scaling Laws.”  Here the speaker points to figure one that shows “Interpolation” and “Extrapolation”.  He points out in this paper, they are somewhat self-aware of the fact that this relationship that they’ve found, this model that they have fit into these datapoints is high and has high validity inside this range.  Interpolating, he continues, “You can say it is pretty accurate, but as soon as you start extrapolating, you don’t know that for sure.”  He likes that they called it a scaling law and calls it outrageous.\n",
            "\n",
            "The person on the video then explains how to get the summary of a paper. “All you have to do is read the abstract and look at the first figure and that will give you 80% of the paper.”\n",
            "\n",
            "Summary for chunk 3:\n",
            "Here is a summary of the video:\n",
            "\n",
            "The content creator is talking about information, entropy, and time, and how they all interact with one another. He shows a diagram of diffusion with images of a cat being destroyed and put back together. He moves over to GitHub and shows code, and references it’s source as “Diffusion Beats Autoregressive in Data-Constrained Settings.” The content creator says a lot about how one type of modeling is better for certain projects than another. After a long explanation, the creator finishes speaking.\n",
            "\n",
            "Summary for chunk 4:\n",
            "The video is on the topic of deep learning models with a particular interest in how these models interact in code. The person in the video discussed an issue of computational availability in data centers, comparing it to single data centers from Google. The narrator believes that decentralization does not work in the data world because Nvidia does not care about consumer GPUs anymore. The video also discussed how the topic of data augmentation leads to ideas of noise, the topic of over fitting, and even mentions data that can be helpful with the data set augmentation. The video host gives a summary that discusses diffusion models and suggests videos for people to watch if they are more interested in that aspect. Lastly, the content creator explains he is getting a bit off-topic and makes a joke about how to turn his viewers in to “Dune Navigators”.\n",
            "\n",
            "Summary for chunk 5:\n",
            "Here is a summary of the video provided.\n",
            "\n",
            "A man reviews two papers. The first paper is titled “Diffusion Beats Autoregressive in Data-Constrained Settings”, and the second is titled “Distillation Scaling Laws”. He states that scaling law papers should not be taken at face value, but rather as inspiration that can be interpolated for use, but not extrapolated too far. \n",
            "\n",
            "He then gives shout-outs to the viewers and ends the stream.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Add Vertex AI imports\n",
        "from google.cloud.aiplatform import init\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "\n",
        "def score_relevancy_gemini(user_query: str, summaries: list[str], model_name: str = \"text-embedding-004\") -> list[float]:\n",
        "    \"\"\"\n",
        "    Calculates relevancy scores for a list of summaries based on a user query\n",
        "    using Gemini embedding and cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        user_query: The user's query string.\n",
        "        summaries: A list of summary strings.\n",
        "        model_name: The name of the Gemini embedding model to use (default is text-embedding-004).\n",
        "\n",
        "    Returns:\n",
        "        A list of cosine similarity scores, one for each summary.\n",
        "    \"\"\"\n",
        "    # Initialize Vertex AI (if not already initialized)\n",
        "    init(project=\"mrc-quant-ml\", location=\"us-central1\")\n",
        "\n",
        "    # Get the embedding model\n",
        "    embedding_model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "\n",
        "    # Embed the user query and each summary using Vertex AI\n",
        "    query_embedding = embedding_model.get_embeddings([user_query])[0].values\n",
        "    summary_embeddings = [embedding.values for embedding in embedding_model.get_embeddings(summaries)]\n",
        "\n",
        "    # Calculate the cosine similarity between the query embedding and each summary embedding\n",
        "    # cosine_similarity expects a 2D array for the first argument, so reshape the query embedding\n",
        "    similarity_scores = cosine_similarity([query_embedding], summary_embeddings)\n",
        "\n",
        "    # The result of cosine_similarity is a 2D array, so flatten it to get a list of scores\n",
        "    return similarity_scores[0].tolist()\n",
        "\n",
        "# Example usage\n",
        "user_query = \"diffusion models\"\n",
        "summaries = all_summaries # Assuming all_summaries is available from previous execution\n",
        "relevancy_scores_gemini = score_relevancy_gemini(user_query, summaries)\n",
        "print(f\"Relevancy scores using Gemini embedding for query '{user_query}': {relevancy_scores_gemini}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS22G8Wa99Eq",
        "outputId": "b7e852b1-43d5-43ae-863f-d23b2ba59331"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevancy scores using Gemini embedding for query 'diffusion models': [0.5230532052217526, 0.4668439884359802, 0.5986543923223047, 0.5457568783517894, 0.5281214862302737]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "el65lj5j-Bhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53a0d305",
        "outputId": "7b742875-c100-4b42-e456-910367bbb5a6"
      },
      "source": [
        "def generate_answer_from_top_chunks(user_query: str, summaries: list[str], relevancy_scores: list[float], k: int = 2):\n",
        "    \"\"\"\n",
        "    Generates an answer to the user query based on the top k most relevant summaries.\n",
        "\n",
        "    Args:\n",
        "        user_query: The user's query string.\n",
        "        summaries: A list of summary strings.\n",
        "        relevancy_scores: A list of relevancy scores corresponding to the summaries.\n",
        "        k: The number of top summaries to use (default is 2).\n",
        "\n",
        "    Returns:\n",
        "        A generated answer based on the top k summaries.\n",
        "    \"\"\"\n",
        "    # Pair summaries with their scores and sort by score in descending order\n",
        "    scored_summaries = sorted(zip(summaries, relevancy_scores), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Select the top k summaries\n",
        "    top_k_summaries = [summary for summary, score in scored_summaries[:k]]\n",
        "\n",
        "    # Combine the top k summaries into a single text\n",
        "    combined_summaries = \"\\n\\n\".join(top_k_summaries)\n",
        "\n",
        "    # Use Gemini API to generate an answer based on the combined summaries\n",
        "    client = Client(\n",
        "        vertexai=True,\n",
        "        project=\"mrc-quant-ml\",\n",
        "        location=\"us-central1\",\n",
        "    )\n",
        "\n",
        "    prompt = f\"Based on the following summaries, answer the question: '{user_query}'\\n\\nSummaries:\\n{combined_summaries}\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        contents=[prompt],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Example usage with k=2 (you can change k as needed)\n",
        "k_value = 2\n",
        "generated_answer = generate_answer_from_top_chunks(user_query, all_summaries, relevancy_scores_gemini, k=k_value)\n",
        "print(f\"\\nGenerated answer based on top {k_value} chunks:\\n{generated_answer}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated answer based on top 2 chunks:\n",
            "Based on the summaries, diffusion models are:\n",
            "\n",
            "*   **A type of deep learning model:** Used in code and related to data augmentation, noise, and overfitting.\n",
            "*   **Related to information, entropy, and time:** Possibly involving a process of destruction and reconstruction (like the \"cat being destroyed and put back together\" image).\n",
            "*   **A focus of specific research:** Referenced by the title \"Diffusion Beats Autoregressive in Data-Constrained Settings,\" implying they are an alternative to autoregressive models and potentially better in certain situations.\n",
            "*   **A separate topic of interest:** The video host suggests further viewing for those specifically interested in diffusion models, indicating they are a distinct area within the broader topic.\n"
          ]
        }
      ]
    }
  ]
}