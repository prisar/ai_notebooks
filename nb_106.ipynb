{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnWgROZ26Z4qZvszp/yrI2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisar/ai_notebooks/blob/main/nb_106.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCw8wUN3_EOK"
      },
      "outputs": [],
      "source": [
        "# Authentication and service account setup\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import os\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set project ID\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'mrc-quant-ml'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q google-genai google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "XP9Zlcodk_zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize\n",
        "from google.genai import Client\n",
        "from google.genai.types import Part, VideoMetadata, FileData\n",
        "from google.cloud import storage\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import nest_asyncio\n",
        "import time # Import time module for delays\n",
        "import moviepy.editor as mp # Import moviepy for video duration\n",
        "\n",
        "\n",
        "def summarize_video_chunk(video_uri: str, start_offset: str, end_offset: str, prompt: str = \"Analyze this video and provide a summary.\"):\n",
        "    \"\"\"Summarizes a video chunk using the Gemini API.\"\"\"\n",
        "    client = Client(\n",
        "        vertexai=True,\n",
        "        project=\"mrc-quant-ml\",\n",
        "        location=\"us-central1\",\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        contents=[\n",
        "            Part(\n",
        "                video_metadata=VideoMetadata(\n",
        "                    fps=1,\n",
        "                    start_offset=start_offset,\n",
        "                    end_offset=end_offset\n",
        "                ),\n",
        "                file_data=FileData(\n",
        "                    file_uri=video_uri,\n",
        "                    mime_type=\"video/mp4\",\n",
        "                ),\n",
        "            ),\n",
        "            prompt\n",
        "        ],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Function to get video duration\n",
        "async def get_video_duration(video_uri: str) -> int:\n",
        "    \"\"\"Gets the duration of a video from a GCS URI.\"\"\"\n",
        "    try:\n",
        "        # Assuming the video is in a GCS bucket\n",
        "        client = storage.Client()\n",
        "        bucket_name, blob_name = video_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
        "        bucket = client.get_bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        # Download the video temporarily to get duration (consider optimizing this)\n",
        "        temp_file = f\"/tmp/{blob_name.split('/')[-1]}\"\n",
        "        blob.download_to_filename(temp_file)\n",
        "        clip = mp.VideoFileClip(temp_file)\n",
        "        duration = int(clip.duration)\n",
        "        os.remove(temp_file) # Clean up the temporary file\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting video duration: {e}\")\n",
        "        return 0 # Return 0 or raise an error based on desired behavior\n",
        "\n",
        "# Batch processing optimization\n",
        "async def process_video_chunks_parallel(video_uri: str, chunk_duration_minutes: int = 30, max_workers: int = 4, delay_seconds: int = 1):\n",
        "    \"\"\"Process video chunks in parallel for better throughput with 30-minute intervals and a delay between API calls.\"\"\"\n",
        "    chunk_duration = chunk_duration_minutes * 60 # Convert minutes to seconds\n",
        "\n",
        "    total_duration = 7302 # await get_video_duration(video_uri)\n",
        "    print(f\"Total video duration: {total_duration} seconds\")\n",
        "    if total_duration == 0:\n",
        "        print(\"Could not get video duration. Aborting processing.\")\n",
        "        return []\n",
        "\n",
        "    chunks = [(i, min(i + chunk_duration, total_duration))\n",
        "              for i in range(0, total_duration, chunk_duration)]\n",
        "\n",
        "    async def process_chunk_with_delay(start, end):\n",
        "        \"\"\"Helper function to process a chunk with a delay and return summary and metadata.\"\"\"\n",
        "        summary = summarize_video_chunk(video_uri, f\"{start}s\", f\"{end}s\")\n",
        "        await asyncio.sleep(delay_seconds) # Add delay between calls\n",
        "        return {\"start\": start, \"end\": end, \"summary\": summary}\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        tasks = [\n",
        "            loop.run_in_executor(\n",
        "                executor,\n",
        "                asyncio.run, # Use asyncio.run to run the async helper function\n",
        "                process_chunk_with_delay(start, end)\n",
        "            ) for start, end in chunks\n",
        "        ]\n",
        "\n",
        "        chunk_results = await asyncio.gather(*tasks)\n",
        "\n",
        "    return chunk_results # Return list of dictionaries with start, end, and summary\n",
        "\n",
        "# Example usage with error handling\n",
        "video_uri = \"gs://mrc-quant-ml-video-analysis/videoplayback.mp4\"\n",
        "\n",
        "# Example of how to use the parallel processing function\n",
        "\n",
        "nest_asyncio.apply() # Apply this if running in Colab\n",
        "\n",
        "try:\n",
        "    # Add delay_seconds parameter to control delay\n",
        "    video_chunks_with_summaries = asyncio.run(process_video_chunks_parallel(video_uri, chunk_duration_minutes=30, delay_seconds=5))\n",
        "    for i, chunk_info in enumerate(video_chunks_with_summaries):\n",
        "        print(f\"Chunk {i+1} (Start: {chunk_info['start']}s, End: {chunk_info['end']}s):\\n{chunk_info['summary']}\\n\")\n",
        "    print(f\"Video chunks with summaries: {video_chunks_with_summaries}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during parallel processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2ITVO3WlAxv",
        "outputId": "bf64dcec-4372-4a1f-b31e-88f1710b0d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total video duration: 7302 seconds\n",
            "Chunk 1 (Start: 0s, End: 1800s):\n",
            "Sure thing! Here is a summary of the video:\n",
            "\n",
            "The video is a live stream by “hu-po” where he goes over the topic of “Diffusion Beats Autoregressive.” The video begins with testing live streaming, followed by hu-po giving his regular disclaimer. \n",
            "\n",
            "Hu-po briefly talks about the thumbnail, and then moves into discussing a paper from the title, which explores model architectures in OpenAI’s ChatGPT. The content revolves around large language models (LLM) and how AI compares two dominant models. Hu-po is critical about the fact that LLM has been used to train internet data over the past couple of years.\n",
            "\n",
            "Chunk 2 (Start: 1800s, End: 3600s):\n",
            "Here is a summary of the video:\n",
            "The speaker is going over papers about scaling laws for AI models. He points out that a paper he is looking at has extrapolated data to model sizes for which they have no data.\n",
            "Next, the speaker finds an Apple paper called “Distillation Scaling Laws” that correctly identifies which parts of their plots are interpolations and which are extrapolations. He shows and analyzes the primary chart of this paper. \n",
            "Then, the speaker gives a quick rule for understanding a paper in 30 seconds: read the abstract and look at the first figure. \n",
            "After this, he goes back to the Diffusion and Autoregressive in Data Constrained Settings paper and explains that it is showing the inverse of the data and validation loss versus FLOPS. \n",
            "After the analysis, the presenter opens Google and shows an example image about data augmentation in neural networks. Finally, he gives an overview of diffusion models by playing a video and describing the process and points out that some have a lot of the key data points, but a lot of it is extrapolated.\n",
            "\n",
            "Chunk 3 (Start: 3600s, End: 5400s):\n",
            "Here is a summary of the video.\n",
            "\n",
            "The speaker compares and contrasts autoregressive models and diffusion models, referring to a study of the two and their data-restrained settings. He then suggests that if diffusion models tend to be better and less biased, than one might ask why autoregressive models are primarily used. One reason is that these auto regressive models are sequential, moving down elements of the sequence one by one, which biases the model. However, since what we are trying to model is language, and language is developed from humans existing in spacetime, there is inherently time in the model.\n",
            "\n",
            "To demonstrate what this means, the speaker then considers imagery. He states images are “closer” to space than time, suggesting diffusion may be more effective in images than with language. He returns to another chart, where we can see the loss as the number of epochs gets greater. For autoregressive models, this has diminishing returns and eventually gets stupid. Again, diffusion models do not demonstrate this pattern. \n",
            "\n",
            "To this end, he suggests what Prime Intellect is trying to accomplish. In this goal, they are going to take a look at a Di Locato (2022) paper and find the basic model for distributed training across compute that are spread out geographically, and build an AI startup based on this knowledge. The flaw, the speaker argues, is that back in 2022, this was a good idea. But GPUs have exploded exponentially since then. Also, that one of the points of having GPUs is to increase computing power. \n",
            "\n",
            "He asks the question which is more important, researchers or more GPUs, referring back to Elan, whose approach was to get more of the GPUs rather than pay for the celebrity scientists and researchers. With high amounts of data (like with language), auto regressive and autoregressive frameworks would need more compute. So in the end, researchers and computing power come into the equation. Lastly, he states we’re likely just going to loop into information theory. He ends on the fact that there are more people good at math and programming than there are the data or funds to pay those people.\n",
            "\n",
            "Chunk 4 (Start: 5400s, End: 7200s):\n",
            "Here is a summary of the video:\n",
            "\n",
            "The video discusses a paper titled, “Diffusion Beats Autoregressive in Data-Constrained Settings.” It talks about the advantages, or implicit data augmentation, of diffusion models compared to autoregressive models. As AI continues to develop, the relative size of the compute available in a decentralized world versus the compute available in centralized data centers is becoming more divergent. As a result of this shift in NVIDIA’s strategy, decentalization is more difficult if NVIDIA doesn’t care about the gaming chips.\n",
            "\n",
            "He moves on to talk about a future where one company, XAI, builds giant-ass data centers that they do not allow anyone else to use. He then discusses the current AI landscape and touches on a new wave of super teams and the concepts in their work. He concludes by discussing and promoting the YouTube channel, @WatchLabVideos, where there are many educational resources that focus on diffusion models.\n",
            "\n",
            "Chunk 5 (Start: 7200s, End: 7302s):\n",
            "The video is of a man speaking about a science paper he is reading. He states that the information in the paper should be used as intuition and not extrapolated. At the end, the man shouts out names of viewers and wishes everyone a good weekend.\n",
            "\n",
            "Video chunks with summaries: [{'start': 0, 'end': 1800, 'summary': 'Sure thing! Here is a summary of the video:\\n\\nThe video is a live stream by “hu-po” where he goes over the topic of “Diffusion Beats Autoregressive.” The video begins with testing live streaming, followed by hu-po giving his regular disclaimer. \\n\\nHu-po briefly talks about the thumbnail, and then moves into discussing a paper from the title, which explores model architectures in OpenAI’s ChatGPT. The content revolves around large language models (LLM) and how AI compares two dominant models. Hu-po is critical about the fact that LLM has been used to train internet data over the past couple of years.'}, {'start': 1800, 'end': 3600, 'summary': 'Here is a summary of the video:\\nThe speaker is going over papers about scaling laws for AI models. He points out that a paper he is looking at has extrapolated data to model sizes for which they have no data.\\nNext, the speaker finds an Apple paper called “Distillation Scaling Laws” that correctly identifies which parts of their plots are interpolations and which are extrapolations. He shows and analyzes the primary chart of this paper.\\xa0\\nThen, the speaker gives a quick rule for understanding a paper in 30 seconds: read the abstract and look at the first figure.\\xa0\\nAfter this, he goes back to the Diffusion and Autoregressive in Data Constrained Settings paper and explains that it is showing the inverse of the data and validation loss versus FLOPS.\\xa0\\nAfter the analysis, the presenter opens Google and shows an example image about data augmentation in neural networks. Finally, he gives an overview of diffusion models by playing a video and describing the process and points out that some have a lot of the key data points, but a lot of it is extrapolated.'}, {'start': 3600, 'end': 5400, 'summary': 'Here is a summary of the video.\\n\\nThe speaker compares and contrasts autoregressive models and diffusion models, referring to a study of the two and their data-restrained settings. He then suggests that if diffusion models tend to be better and less biased, than one might ask why autoregressive models are primarily used. One reason is that these auto regressive models are sequential, moving down elements of the sequence one by one, which biases the model. However, since what we are trying to model is language, and language is developed from humans existing in spacetime, there is inherently time in the model.\\n\\nTo demonstrate what this means, the speaker then considers imagery. He states images are “closer” to space than time, suggesting diffusion may be more effective in images than with language. He returns to another chart, where we can see the loss as the number of epochs gets greater. For autoregressive models, this has diminishing returns and eventually gets stupid. Again, diffusion models do not demonstrate this pattern. \\n\\nTo this end, he suggests what Prime Intellect is trying to accomplish. In this goal, they are going to take a look at a Di Locato (2022) paper and find the basic model for distributed training across compute that are spread out geographically, and build an AI startup based on this knowledge. The flaw, the speaker argues, is that back in 2022, this was a good idea. But GPUs have exploded exponentially since then. Also, that one of the points of having GPUs is to increase computing power. \\n\\nHe asks the question which is more important, researchers or more GPUs, referring back to Elan, whose approach was to get more of the GPUs rather than pay for the celebrity scientists and researchers. With high amounts of data (like with language), auto regressive and autoregressive frameworks would need more compute. So in the end, researchers and computing power come into the equation. Lastly, he states we’re likely just going to loop into information theory. He ends on the fact that there are more people good at math and programming than there are the data or funds to pay those people.'}, {'start': 5400, 'end': 7200, 'summary': 'Here is a summary of the video:\\n\\nThe video discusses a paper titled, “Diffusion Beats Autoregressive in Data-Constrained Settings.” It talks about the advantages, or implicit data augmentation, of diffusion models compared to autoregressive models. As AI continues to develop, the relative size of the compute available in a decentralized world versus the compute available in centralized data centers is becoming more divergent. As a result of this shift in NVIDIA’s strategy, decentalization is more difficult if NVIDIA doesn’t care about the gaming chips.\\n\\nHe moves on to talk about a future where one company, XAI, builds giant-ass data centers that they do not allow anyone else to use. He then discusses the current AI landscape and touches on a new wave of super teams and the concepts in their work. He concludes by discussing and promoting the YouTube channel, @WatchLabVideos, where there are many educational resources that focus on diffusion models.'}, {'start': 7200, 'end': 7302, 'summary': 'The video is of a man speaking about a science paper he is reading. He states that the information in the paper should be used as intuition and not extrapolated. At the end, the man shouts out names of viewers and wishes everyone a good weekend.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Add Vertex AI imports\n",
        "from google.cloud.aiplatform import init\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "\n",
        "def score_relevancy_gemini(user_query: str, summaries: list[str], model_name: str = \"text-embedding-004\") -> list[float]:\n",
        "    \"\"\"\n",
        "    Calculates relevancy scores for a list of summaries based on a user query\n",
        "    using Gemini embedding and cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        user_query: The user's query string.\n",
        "        summaries: A list of summary strings.\n",
        "        model_name: The name of the Gemini embedding model to use (default is text-embedding-004).\n",
        "\n",
        "    Returns:\n",
        "        A list of cosine similarity scores, one for each summary.\n",
        "    \"\"\"\n",
        "    # Initialize Vertex AI (if not already initialized)\n",
        "    init(project=\"mrc-quant-ml\", location=\"us-central1\")\n",
        "\n",
        "    # Get the embedding model\n",
        "    embedding_model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "\n",
        "    # Embed the user query and each summary using Vertex AI\n",
        "    query_embedding = embedding_model.get_embeddings([user_query])[0].values\n",
        "    summary_embeddings = [embedding.values for embedding in embedding_model.get_embeddings(summaries)]\n",
        "\n",
        "    # Calculate the cosine similarity between the query embedding and each summary embedding\n",
        "    # cosine_similarity expects a 2D array for the first argument, so reshape the query embedding\n",
        "    similarity_scores = cosine_similarity([query_embedding], summary_embeddings)\n",
        "\n",
        "    # The result of cosine_similarity is a 2D array, so flatten it to get a list of scores\n",
        "    return similarity_scores[0].tolist()\n",
        "\n",
        "# Example usage\n",
        "user_query = \"diffusion models\"\n",
        "summaries = [chunk['summary'] for chunk in video_chunks_with_summaries] # Extract summaries from the list of dictionaries\n",
        "relevancy_scores_gemini = score_relevancy_gemini(user_query, summaries)\n",
        "print(f\"Relevancy scores using Gemini embedding for query '{user_query}': {relevancy_scores_gemini}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u2qKqvxlhIg",
        "outputId": "c48ac433-9ef2-4827-8ea6-8a8508ca6281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevancy scores using Gemini embedding for query 'diffusion models': [0.5578611017348896, 0.5802670089981152, 0.5742812468244225, 0.5929148443269238, 0.28378637212218516]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_chunks_with_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um98ipDonRBF",
        "outputId": "2b6c316f-84fa-4d4c-c055-6d5346c91491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'start': 0,\n",
              "  'end': 1800,\n",
              "  'summary': 'Sure thing! Here is a summary of the video:\\n\\nThe video is a live stream by “hu-po” where he goes over the topic of “Diffusion Beats Autoregressive.” The video begins with testing live streaming, followed by hu-po giving his regular disclaimer. \\n\\nHu-po briefly talks about the thumbnail, and then moves into discussing a paper from the title, which explores model architectures in OpenAI’s ChatGPT. The content revolves around large language models (LLM) and how AI compares two dominant models. Hu-po is critical about the fact that LLM has been used to train internet data over the past couple of years.'},\n",
              " {'start': 1800,\n",
              "  'end': 3600,\n",
              "  'summary': 'Here is a summary of the video:\\nThe speaker is going over papers about scaling laws for AI models. He points out that a paper he is looking at has extrapolated data to model sizes for which they have no data.\\nNext, the speaker finds an Apple paper called “Distillation Scaling Laws” that correctly identifies which parts of their plots are interpolations and which are extrapolations. He shows and analyzes the primary chart of this paper.\\xa0\\nThen, the speaker gives a quick rule for understanding a paper in 30 seconds: read the abstract and look at the first figure.\\xa0\\nAfter this, he goes back to the Diffusion and Autoregressive in Data Constrained Settings paper and explains that it is showing the inverse of the data and validation loss versus FLOPS.\\xa0\\nAfter the analysis, the presenter opens Google and shows an example image about data augmentation in neural networks. Finally, he gives an overview of diffusion models by playing a video and describing the process and points out that some have a lot of the key data points, but a lot of it is extrapolated.'},\n",
              " {'start': 3600,\n",
              "  'end': 5400,\n",
              "  'summary': 'Here is a summary of the video.\\n\\nThe speaker compares and contrasts autoregressive models and diffusion models, referring to a study of the two and their data-restrained settings. He then suggests that if diffusion models tend to be better and less biased, than one might ask why autoregressive models are primarily used. One reason is that these auto regressive models are sequential, moving down elements of the sequence one by one, which biases the model. However, since what we are trying to model is language, and language is developed from humans existing in spacetime, there is inherently time in the model.\\n\\nTo demonstrate what this means, the speaker then considers imagery. He states images are “closer” to space than time, suggesting diffusion may be more effective in images than with language. He returns to another chart, where we can see the loss as the number of epochs gets greater. For autoregressive models, this has diminishing returns and eventually gets stupid. Again, diffusion models do not demonstrate this pattern. \\n\\nTo this end, he suggests what Prime Intellect is trying to accomplish. In this goal, they are going to take a look at a Di Locato (2022) paper and find the basic model for distributed training across compute that are spread out geographically, and build an AI startup based on this knowledge. The flaw, the speaker argues, is that back in 2022, this was a good idea. But GPUs have exploded exponentially since then. Also, that one of the points of having GPUs is to increase computing power. \\n\\nHe asks the question which is more important, researchers or more GPUs, referring back to Elan, whose approach was to get more of the GPUs rather than pay for the celebrity scientists and researchers. With high amounts of data (like with language), auto regressive and autoregressive frameworks would need more compute. So in the end, researchers and computing power come into the equation. Lastly, he states we’re likely just going to loop into information theory. He ends on the fact that there are more people good at math and programming than there are the data or funds to pay those people.'},\n",
              " {'start': 5400,\n",
              "  'end': 7200,\n",
              "  'summary': 'Here is a summary of the video:\\n\\nThe video discusses a paper titled, “Diffusion Beats Autoregressive in Data-Constrained Settings.” It talks about the advantages, or implicit data augmentation, of diffusion models compared to autoregressive models. As AI continues to develop, the relative size of the compute available in a decentralized world versus the compute available in centralized data centers is becoming more divergent. As a result of this shift in NVIDIA’s strategy, decentalization is more difficult if NVIDIA doesn’t care about the gaming chips.\\n\\nHe moves on to talk about a future where one company, XAI, builds giant-ass data centers that they do not allow anyone else to use. He then discusses the current AI landscape and touches on a new wave of super teams and the concepts in their work. He concludes by discussing and promoting the YouTube channel, @WatchLabVideos, where there are many educational resources that focus on diffusion models.'},\n",
              " {'start': 7200,\n",
              "  'end': 7302,\n",
              "  'summary': 'The video is of a man speaking about a science paper he is reading. He states that the information in the paper should be used as intuition and not extrapolated. At the end, the man shouts out names of viewers and wishes everyone a good weekend.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Add Vertex AI imports\n",
        "from google.cloud.aiplatform import init\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from google.genai.types import Part, VideoMetadata, FileData\n",
        "\n",
        "\n",
        "def generate_answer_from_top_chunks(user_query: str, video_chunks: list[dict], relevancy_scores: list[float], k: int = 2, video_uri: str = \"gs://mrc-quant-ml-video-analysis/videoplayback.mp4\"):\n",
        "    \"\"\"\n",
        "    Generates an answer to the user query based on the top k most relevant video chunks.\n",
        "\n",
        "    Args:\n",
        "        user_query: The user's query string.\n",
        "        video_chunks: A list of dictionaries, each containing 'start', 'end', and 'summary' of a video chunk.\n",
        "        relevancy_scores: A list of relevancy scores corresponding to the video chunks.\n",
        "        k: The number of top chunks to use (default is 2).\n",
        "        video_uri: The GCS URI of the video.\n",
        "\n",
        "    Returns:\n",
        "        A generated answer based on the top k chunks.\n",
        "    \"\"\"\n",
        "    # Pair chunks with their scores and sort by score in descending order\n",
        "    scored_chunks = sorted(zip(video_chunks, relevancy_scores), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Select the top k chunks\n",
        "    top_k_chunks = [chunk for chunk, score in scored_chunks[:k]]\n",
        "\n",
        "    # Prepare the content for the Gemini API call\n",
        "    contents = [user_query]\n",
        "    for chunk in top_k_chunks:\n",
        "        contents.append(\n",
        "            Part(\n",
        "                video_metadata=VideoMetadata(\n",
        "                    fps=0.1,\n",
        "                    start_offset=f\"{chunk['start']}s\",\n",
        "                    end_offset=f\"{chunk['end']}s\"\n",
        "                ),\n",
        "                file_data=FileData(\n",
        "                    file_uri=video_uri,\n",
        "                    mime_type=\"video/mp4\",\n",
        "                ),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Use Gemini API to generate an answer based on the top k chunks\n",
        "    client = Client(\n",
        "        vertexai=True,\n",
        "        project=\"mrc-quant-ml\",\n",
        "        location=\"us-central1\",\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        contents=contents,\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Example usage with k=2 (you can change k as needed)\n",
        "k_value = 1\n",
        "user_query = \"why diffusion models are better and at what time it was discussed\"\n",
        "# Pass video_chunks_with_summaries directly to the function\n",
        "generated_answer = generate_answer_from_top_chunks(user_query, video_chunks_with_summaries, relevancy_scores_gemini, k=k_value, video_uri=video_uri)\n",
        "print(f\"\\nGenerated answer based on top {k_value} chunks:\\n{generated_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynodPEL-l0dr",
        "outputId": "c199b24a-f2d4-46bf-8190-26b3039007d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated answer based on top 1 chunks:\n",
            "At [00:10:06], the video discussed why diffusion models are superior because they offer increased data augmentation due to the implicit noise addition, leading to more efficient training.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}