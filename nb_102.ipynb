{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgaeamJTs+A5Jaq2gTDWC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisar/ai_notebooks/blob/main/nb_102.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWwXZV8ftNlX"
      },
      "outputs": [],
      "source": [
        "# Authentication and service account setup\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import os\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set project ID\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'mrc-quant-ml'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q google-genai google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "7Vm6-3QMyTr1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize\n",
        "from google.genai import Client\n",
        "from google.genai.types import Part, VideoMetadata, FileData\n",
        "from google.cloud import storage\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import nest_asyncio\n",
        "import time # Import time module for delays\n",
        "import moviepy.editor as mp # Import moviepy for video duration\n",
        "\n",
        "\n",
        "def summarize_video_chunk(video_uri: str, start_offset: str, end_offset: str, prompt: str = \"Analyze this video and provide a summary.\"):\n",
        "    \"\"\"Summarizes a video chunk using the Gemini API.\"\"\"\n",
        "    client = Client(\n",
        "        vertexai=True,\n",
        "        project=\"mrc-quant-ml\",\n",
        "        location=\"us-central1\",\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        contents=[\n",
        "            Part(\n",
        "                video_metadata=VideoMetadata(\n",
        "                    fps=1,\n",
        "                    start_offset=start_offset,\n",
        "                    end_offset=end_offset\n",
        "                ),\n",
        "                file_data=FileData(\n",
        "                    file_uri=video_uri,\n",
        "                    mime_type=\"video/mp4\",\n",
        "                ),\n",
        "            ),\n",
        "            prompt\n",
        "        ],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Function to get video duration\n",
        "async def get_video_duration(video_uri: str) -> int:\n",
        "    \"\"\"Gets the duration of a video from a GCS URI.\"\"\"\n",
        "    try:\n",
        "        # Assuming the video is in a GCS bucket\n",
        "        client = storage.Client()\n",
        "        bucket_name, blob_name = video_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
        "        bucket = client.get_bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        # Download the video temporarily to get duration (consider optimizing this)\n",
        "        temp_file = f\"/tmp/{blob_name.split('/')[-1]}\"\n",
        "        blob.download_to_filename(temp_file)\n",
        "        clip = mp.VideoFileClip(temp_file)\n",
        "        duration = int(clip.duration)\n",
        "        os.remove(temp_file) # Clean up the temporary file\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting video duration: {e}\")\n",
        "        return 0 # Return 0 or raise an error based on desired behavior\n",
        "\n",
        "# Batch processing optimization\n",
        "async def process_video_chunks_parallel(video_uri: str, chunk_duration_minutes: int = 30, max_workers: int = 4, delay_seconds: int = 1):\n",
        "    \"\"\"Process video chunks in parallel for better throughput with 30-minute intervals and a delay between API calls.\"\"\"\n",
        "    chunk_duration = chunk_duration_minutes * 60 # Convert minutes to seconds\n",
        "\n",
        "    total_duration = 7302 # await get_video_duration(video_uri)\n",
        "    print(f\"Total video duration: {total_duration} seconds\")\n",
        "    if total_duration == 0:\n",
        "        print(\"Could not get video duration. Aborting processing.\")\n",
        "        return []\n",
        "\n",
        "    chunks = [(i, min(i + chunk_duration, total_duration))\n",
        "              for i in range(0, total_duration, chunk_duration)]\n",
        "\n",
        "    def run_summarize_chunk(start, end):\n",
        "      \"\"\"Helper function to run the summarize_video_chunk coroutine.\"\"\"\n",
        "      return asyncio.run(process_chunk_with_delay(start, end))\n",
        "\n",
        "    async def process_chunk_with_delay(start, end):\n",
        "        \"\"\"Helper function to process a chunk with a delay.\"\"\"\n",
        "        summary = summarize_video_chunk(video_uri, f\"{start}s\", f\"{end}s\")\n",
        "        await asyncio.sleep(delay_seconds) # Add delay between calls\n",
        "        return summary\n",
        "\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        tasks = [\n",
        "            loop.run_in_executor(\n",
        "                executor,\n",
        "                run_summarize_chunk,\n",
        "                start,\n",
        "                end\n",
        "            ) for start, end in chunks\n",
        "        ]\n",
        "\n",
        "        summaries = await asyncio.gather(*tasks)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Example usage with error handling\n",
        "video_uri = \"gs://mrc-quant-ml-video-analysis/videoplayback.mp4\"\n",
        "\n",
        "# Example of how to use the parallel processing function\n",
        "\n",
        "nest_asyncio.apply() # Apply this if running in Colab\n",
        "\n",
        "try:\n",
        "    # Add delay_seconds parameter to control delay\n",
        "    all_summaries = asyncio.run(process_video_chunks_parallel(video_uri, chunk_duration_minutes=30, delay_seconds=5))\n",
        "    for i, summary in enumerate(all_summaries):\n",
        "        print(f\"Summary for chunk {i+1}:\\n{summary}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during parallel processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb1rQoUzyWT3",
        "outputId": "0a7d9634-5aad-4e4b-e576-f559df358f2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total video duration: 7302 seconds\n",
            "Summary for chunk 1:\n",
            "Alright, I can provide a video summary:\n",
            "In this video, the host, \"hu-po,\" is doing a live coding and analysis stream, which he titles \"Diffusion vs. Autoregressive.\" His focus is a paper from Carnegie Mellon University, about Diffusion models beating Autoregressive ones in data-constrained settings. He plans to review the abstract of the paper, which came out on July 21, 2025.\n",
            "\n",
            "He notes that the thumbnail for the video, featuring illustrated cats, was created in OpenAI's GPT, though GPT made a misspelling. He says these sorts of errors are interesting. \n",
            "\n",
            "He says he'd like to talk about Scaling Laws a bit. Hu-po says you can always generate more data but that comes at a cost of time.\n",
            "\n",
            "Hu-po shows a diagram that he likes, then he makes an offhand comment about \"British accents making people sound smarter,\" before explaining that the issue is \"overfitting.\"\n",
            "\n",
            "Hu-po cites a study that involved training on synthetic textbooks leading to better results. The core point is that the action diffusion transformer is important, and diffusion models help. He shows a graph about the Pareto frontier of validation Loss.\n",
            "\n",
            "Summary for chunk 2:\n",
            "Okay, I can do that. Here is a summary of the video.\n",
            "\n",
            "The host shows his audience multiple articles on the topic of AI learning algorithms. In the beginning of his video, he points to a chart demonstrating model size relative to number of training floating point operations, and explains that the points are models of sizes 10M and 10B parameters. A later chart is shown, which the host says is better at explaining the difference between interpolation and extrapolation.Â \n",
            "\n",
            "To further illustrate this, he transitions to a YouTube video visually depicting the Diffusion model, explaining that training is performed while removing noise step by step.\n",
            "\n",
            "Summary for chunk 3:\n",
            "Okay! Here is a summary of the video.\n",
            "\n",
            "To start, the video shows the presenter on the bottom left corner of the screen. The presenter goes on to show three squares with the same cat image within them, each with an arrow pointed at a square to the right of it which has \"Noise\" written below each of the arrows. The presenter discusses how diffusion can allow you to make sure that different eyes are the same color within a image, while autoregressive models would require the user to pick the eye color prior to the other eye color within the image. \n",
            "\n",
            "The video then cuts to the presenter showing a article written by four other people from Carnegie Mellon University and Lambda comparing diffusion models versus autoregressive models. \n",
            "\n",
            "Towards the end, the presenter switches to a deep-mind image of diffusion math being done for the question \"Please provide the solution to this math problem â(81)*(2/3)^2-(15-3)/(2^2)\".\n",
            "\n",
            "That's the summary for this video!\n",
            "\n",
            "Summary for chunk 4:\n",
            "Here is a summary of the video.\n",
            "\n",
            "The vlogger is discussing the data sets for artificial language models and notes that this space has so far been dominated by autoregressive models, but diffusion-based models have emerged in a promising way. He also states that Nvidia doesnât care about consumer GPUs anymore and it will be harder to find compute outside of data centers for training. In the long term, the vloggers speculates that a post human future is one, where there are small groups on large AI data centers who have long given up normal human living.\n",
            "\n",
            "Summary for chunk 5:\n",
            "Here is a summary of the video:\n",
            "\n",
            "The speaker goes over two graphs that compare the performance of diffusion models and autoregressive models in data-constrained settings. He states that the findings may only serve as an intuition piece, but that extrapolation should be avoided. He then thanks viewers for tuning in to his show and tells them to go out and train a diffusion model, plays a loud horn, and ends the broadcast.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFqvr_tX0wdu",
        "outputId": "fddb465b-ccb3-4a73-e867-cc29a3059d04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Alright, I can provide a video summary:\\nIn this video, the host, \"hu-po,\" is doing a live coding and analysis stream, which he titles \"Diffusion vs. Autoregressive.\" His focus is a paper from Carnegie Mellon University, about Diffusion models beating Autoregressive ones in data-constrained settings. He plans to review the abstract of the paper, which came out on July 21, 2025.\\n\\nHe notes that the thumbnail for the video, featuring illustrated cats, was created in OpenAI\\'s GPT, though GPT made a misspelling. He says these sorts of errors are interesting. \\n\\nHe says he\\'d like to talk about Scaling Laws a bit. Hu-po says you can always generate more data but that comes at a cost of time.\\n\\nHu-po shows a diagram that he likes, then he makes an offhand comment about \"British accents making people sound smarter,\" before explaining that the issue is \"overfitting.\"\\n\\nHu-po cites a study that involved training on synthetic textbooks leading to better results. The core point is that the action diffusion transformer is important, and diffusion models help. He shows a graph about the Pareto frontier of validation Loss.',\n",
              " 'Okay, I can do that. Here is a summary of the video.\\n\\nThe host shows his audience multiple articles on the topic of AI learning algorithms. In the beginning of his video, he points to a chart demonstrating model size relative to number of training floating point operations, and explains that the points are models of sizes 10M and 10B parameters. A later chart is shown, which the host says is better at explaining the difference between interpolation and extrapolation.\\xa0\\n\\nTo further illustrate this, he transitions to a YouTube video visually depicting the Diffusion model, explaining that training is performed while removing noise step by step.',\n",
              " 'Okay! Here is a summary of the video.\\n\\nTo start, the video shows the presenter on the bottom left corner of the screen. The presenter goes on to show three squares with the same cat image within them, each with an arrow pointed at a square to the right of it which has \"Noise\" written below each of the arrows. The presenter discusses how diffusion can allow you to make sure that different eyes are the same color within a image, while autoregressive models would require the user to pick the eye color prior to the other eye color within the image. \\n\\nThe video then cuts to the presenter showing a article written by four other people from Carnegie Mellon University and Lambda comparing diffusion models versus autoregressive models. \\n\\nTowards the end, the presenter switches to a deep-mind image of diffusion math being done for the question \"Please provide the solution to this math problem â(81)*(2/3)^2-(15-3)/(2^2)\".\\n\\nThat\\'s the summary for this video!',\n",
              " 'Here is a summary of the video.\\n\\nThe vlogger is discussing the data sets for artificial language models and notes that this space has so far been dominated by autoregressive models, but diffusion-based models have emerged in a promising way. He also states that Nvidia doesnât care about consumer GPUs anymore and it will be harder to find compute outside of data centers for training. In the long term, the vloggers speculates that a post human future is one, where there are small groups on large AI data centers who have long given up normal human living.',\n",
              " 'Here is a summary of the video:\\n\\nThe speaker goes over two graphs that compare the performance of diffusion models and autoregressive models in data-constrained settings. He states that the findings may only serve as an intuition piece, but that extrapolation should be avoided. He then thanks viewers for tuning in to his show and tells them to go out and train a diffusion model, plays a loud horn, and ends the broadcast.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bs4fKMv1RCw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}