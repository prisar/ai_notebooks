{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisar/ai_notebooks/blob/main/nb_090.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCSqIobacbSw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f56035b"
      },
      "source": [
        "# Task\n",
        "Perform a reinforcement learning experiment by building a custom Gridworld environment from scratch. The agent should learn to navigate this 2D maze using a CNN-based Deep Q-Network (DQN). For a bonus, use PyGame or gymnasium to visually render the environment and the agent's learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d3fef01"
      },
      "source": [
        "## Environment setup\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary libraries, including `pygame` for visualization and `tensorflow` for the DQN model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95eb30b3"
      },
      "source": [
        "**Reasoning**:\n",
        "I will install the necessary libraries, `pygame` and `tensorflow`, using pip as instructed. I will combine both installation commands into a single code block for efficiency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37eb04e3",
        "outputId": "12ec7ad7-4e56-43f9-b595-37573155f37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "514006e0"
      },
      "source": [
        "## Gridworld environment\n",
        "\n",
        "### Subtask:\n",
        "Create a custom Gridworld environment class. This class will handle the game logic, including the grid, agent, rewards, and state transitions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c6f0b7d"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now create the `GridworldEnv` class as requested. This class will include the `__init__`, `reset`, `step`, and `render` methods as specified in the instructions. The `__init__` method will set up the grid, agent, goal, and obstacles. The `reset` method will bring the environment to its initial state. The `step` method will handle the agent's actions, state transitions, and rewards. Finally, the `render` method will be a placeholder for now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46ffb5fb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridworldEnv:\n",
        "    def __init__(self, grid_size=(10, 10)):\n",
        "        self.grid_size = grid_size\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [grid_size[0] - 1, grid_size[1] - 1]\n",
        "        self.obstacles = [[1, 1], [1, 2], [2, 1], [2, 2], [5,5], [5,6], [6,5], [6,6]]\n",
        "        self.state = self._get_state()\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.state = self._get_state()\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # Up\n",
        "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
        "        elif action == 1:  # Down\n",
        "            self.agent_pos[0] = min(self.grid_size[0] - 1, self.agent_pos[0] + 1)\n",
        "        elif action == 2:  # Left\n",
        "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
        "        elif action == 3:  # Right\n",
        "            self.agent_pos[1] = min(self.grid_size[1] - 1, self.agent_pos[1] + 1)\n",
        "\n",
        "        self.state = self._get_state()\n",
        "        done = False\n",
        "        reward = -0.1  # Small negative reward for each step\n",
        "\n",
        "        if self.agent_pos == self.goal_pos:\n",
        "            reward = 10.0\n",
        "            done = True\n",
        "        elif self.agent_pos in self.obstacles:\n",
        "            reward = -5.0\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        # Placeholder for rendering\n",
        "        grid = np.zeros(self.grid_size)\n",
        "        grid[self.agent_pos[0], self.agent_pos[1]] = 0.5  # Agent\n",
        "        grid[self.goal_pos[0], self.goal_pos[1]] = 1.0  # Goal\n",
        "        for obs in self.obstacles:\n",
        "            grid[obs[0], obs[1]] = -1.0 # Obstacle\n",
        "        print(grid)\n",
        "\n",
        "\n",
        "    def _get_state(self):\n",
        "        state = np.zeros(self.grid_size)\n",
        "        state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
        "        return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7beb74bb"
      },
      "source": [
        "## Cnn-based dqn model\n",
        "\n",
        "### Subtask:\n",
        "Define and build a Deep Q-Network (DQN) model using a Convolutional Neural Network (CNN) to process the grid-based state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cc4cc92"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now define the DQN model. I will import the necessary layers from TensorFlow and Keras, then create a function that builds the CNN-based model as specified in the instructions. The model will have two convolutional layers, a flatten layer, and two dense layers, with the final layer outputting the Q-values for each action. The model will be compiled with the Adam optimizer and mean squared error loss function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ba205de"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_dqn_model(input_shape, num_actions):\n",
        "    \"\"\"\n",
        "    Creates a Deep Q-Network (DQN) model.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): The shape of the input state.\n",
        "        num_actions (int): The number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.models.Model: The compiled DQN model.\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu')(conv1)\n",
        "\n",
        "    # Flatten the output\n",
        "    flatten = Flatten()(conv2)\n",
        "\n",
        "    # Dense layers\n",
        "    dense1 = Dense(128, activation='relu')(flatten)\n",
        "    output_layer = Dense(num_actions, activation='linear')(dense1)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf58b7f2"
      },
      "source": [
        "## Agent training\n",
        "\n",
        "### Subtask:\n",
        "Implement the DQN training loop, including experience replay and target network updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ab96dd7"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the DQN training loop, including experience replay and target network updates. This involves instantiating the environment and models, creating a target network, and implementing the main training loop with epsilon-greedy action selection, experience replay, and target network updates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "7518ee45",
        "outputId": "a2d3ccbf-2b08-4df1-9956-97be78585ef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0: Total Reward = -5.2, Epsilon = 0.99\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2456646194.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtarget_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_dqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Use argmax on the main network's predictions for the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mmain_q_values_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_dqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mnext_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_q_values_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    498\u001b[0m     ):\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# Create an iterator that yields batches of input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             dataset = tf.data.Dataset.zip(\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             )\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m\"\"\"See `tf.data.Dataset.from_tensors` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    132\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 134\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    730\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    733\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_tensor_conversion.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    277\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m ) -> ops._EagerTensorBase:\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "\n",
        "# Hyperparameters\n",
        "num_episodes = 1000\n",
        "max_steps_per_episode = 100\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay_rate = 0.995\n",
        "min_epsilon = 0.01\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99  # Discount factor\n",
        "replay_buffer_size = 10000\n",
        "batch_size = 32\n",
        "target_update_frequency = 10\n",
        "\n",
        "# Instantiate the environment and the main DQN model\n",
        "env = GridworldEnv()\n",
        "state_shape = (env.grid_size[0], env.grid_size[1], 1) # Add channel dimension\n",
        "num_actions = 4 # Up, Down, Left, Right\n",
        "main_dqn = create_dqn_model(state_shape, num_actions)\n",
        "\n",
        "# Create the target network and copy initial weights\n",
        "target_dqn = create_dqn_model(state_shape, num_actions)\n",
        "target_dqn.set_weights(main_dqn.get_weights())\n",
        "\n",
        "# Experience Replay Buffer\n",
        "replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.expand_dims(state, axis=-1) # Add channel dimension\n",
        "    state = np.expand_dims(state, axis=0) # Add batch dimension\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randrange(num_actions)\n",
        "        else:\n",
        "            q_values = main_dqn.predict(state, verbose=0)\n",
        "            action = np.argmax(q_values[0])\n",
        "\n",
        "        # Take the selected action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.expand_dims(next_state, axis=-1) # Add channel dimension\n",
        "        next_state = np.expand_dims(next_state, axis=0) # Add batch dimension\n",
        "        total_reward += reward\n",
        "\n",
        "        # Store experience in replay buffer\n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Train the main DQN model if buffer is large enough\n",
        "        if len(replay_buffer) > batch_size:\n",
        "            minibatch = random.sample(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "            states = np.vstack(states)\n",
        "            next_states = np.vstack(next_states)\n",
        "\n",
        "            target_q_values = target_dqn.predict(next_states, verbose=0)\n",
        "            # Use argmax on the main network's predictions for the next state\n",
        "            main_q_values_next = main_dqn.predict(next_states, verbose=0)\n",
        "            next_actions = np.argmax(main_q_values_next, axis=1)\n",
        "\n",
        "            # Bellman equation\n",
        "            target_q = rewards + gamma * np.max(target_q_values, axis=1) * (1 - np.array(dones))\n",
        "\n",
        "            # Create target Q-values for the main network\n",
        "            with tf.GradientTape() as tape:\n",
        "                current_q_values = main_dqn(states)\n",
        "                one_hot_actions = tf.one_hot(actions, num_actions)\n",
        "                predicted_q_values = tf.reduce_sum(current_q_values * one_hot_actions, axis=1)\n",
        "                loss = tf.keras.losses.MSE(target_q, predicted_q_values)\n",
        "\n",
        "            grads = tape.gradient(loss, main_dqn.trainable_variables)\n",
        "            main_dqn.optimizer.apply_gradients(zip(grads, main_dqn.trainable_variables))\n",
        "\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)\n",
        "\n",
        "    # Update target network\n",
        "    if episode % target_update_frequency == 0:\n",
        "        target_dqn.set_weights(main_dqn.get_weights())\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode {episode}: Total Reward = {total_reward}, Epsilon = {epsilon:.2f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7907f54d"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "### Subtask:\n",
        "Use `pygame` to visualize the agent's learning process in the Gridworld environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f27bd69"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the pygame library and initialize it. Set up the display window.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6b395c4"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "\n",
        "# Initialize pygame\n",
        "pygame.init()\n",
        "\n",
        "# Set up display dimensions\n",
        "GRID_SIZE = env.grid_size\n",
        "CELL_SIZE = 50\n",
        "DISPLAY_WIDTH = GRID_SIZE[1] * CELL_SIZE\n",
        "DISPLAY_HEIGHT = GRID_SIZE[0] * CELL_SIZE\n",
        "screen = pygame.display.set_mode((DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
        "pygame.display.set_caption(\"Gridworld DQN Training\")\n",
        "\n",
        "# Define colors\n",
        "WHITE = (255, 255, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "BLUE = (0, 0, 255)  # Agent\n",
        "GREEN = (0, 255, 0) # Goal\n",
        "RED = (255, 0, 0) # Obstacle (using RED for obstacles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0430708",
        "outputId": "acac5be6-5fe1-4424-a540-6487beb78936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a0f305d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pygame\n",
        "\n",
        "class GridworldEnv:\n",
        "    def __init__(self, grid_size=(10, 10)):\n",
        "        self.grid_size = grid_size\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [grid_size[0] - 1, grid_size[1] - 1]\n",
        "        self.obstacles = [[1, 1], [1, 2], [2, 1], [2, 2], [5,5], [5,6], [6,5], [6,6]]\n",
        "        self.state = self._get_state()\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.state = self._get_state()\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # Up\n",
        "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
        "        elif action == 1:  # Down\n",
        "            self.agent_pos[0] = min(self.grid_size[0] - 1, self.agent_pos[0] + 1)\n",
        "        elif action == 2:  # Left\n",
        "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
        "        elif action == 3:  # Right\n",
        "            self.agent_pos[1] = min(self.grid_size[1] - 1, self.agent_pos[1] + 1)\n",
        "\n",
        "        self.state = self._get_state()\n",
        "        done = False\n",
        "        reward = -0.1  # Small negative reward for each step\n",
        "\n",
        "        if self.agent_pos == self.goal_pos:\n",
        "            reward = 10.0\n",
        "            done = True\n",
        "        elif self.agent_pos in self.obstacles:\n",
        "            reward = -5.0\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def render(self, screen, cell_size):\n",
        "        screen.fill(WHITE)\n",
        "        for row in range(self.grid_size[0]):\n",
        "            for col in range(self.grid_size[1]):\n",
        "                rect = pygame.Rect(col * cell_size, row * cell_size, cell_size, cell_size)\n",
        "                pygame.draw.rect(screen, BLACK, rect, 1)\n",
        "\n",
        "        # Draw obstacles\n",
        "        for obs in self.obstacles:\n",
        "            rect = pygame.Rect(obs[1] * cell_size, obs[0] * cell_size, cell_size, cell_size)\n",
        "            pygame.draw.rect(screen, RED, rect)\n",
        "\n",
        "        # Draw goal\n",
        "        goal_rect = pygame.Rect(self.goal_pos[1] * cell_size, self.goal_pos[0] * cell_size, cell_size, cell_size)\n",
        "        pygame.draw.rect(screen, GREEN, goal_rect)\n",
        "\n",
        "        # Draw agent\n",
        "        agent_rect = pygame.Rect(self.agent_pos[1] * cell_size, self.agent_pos[0] * cell_size, cell_size, cell_size)\n",
        "        pygame.draw.rect(screen, BLUE, agent_rect)\n",
        "\n",
        "        pygame.display.flip()\n",
        "\n",
        "\n",
        "    def _get_state(self):\n",
        "        state = np.zeros(self.grid_size)\n",
        "        state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
        "        return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "045e9476"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_dqn_model(input_shape, num_actions):\n",
        "    \"\"\"\n",
        "    Creates a Deep Q-Network (DQN) model.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): The shape of the input state.\n",
        "        num_actions (int): The number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.models.Model: The compiled DQN model.\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu')(conv1)\n",
        "\n",
        "    # Flatten the output\n",
        "    flatten = Flatten()(conv2)\n",
        "\n",
        "    # Dense layers\n",
        "    dense1 = Dense(128, activation='relu')(flatten)\n",
        "    output_layer = Dense(num_actions, activation='linear')(dense1)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "052ffe43",
        "outputId": "6066e3da-75e5-4bfd-b779-f1a9f8380b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.11.13)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "Episode 0: Total Reward = -6.0, Epsilon = 0.99\n",
            "Episode 10: Total Reward = -5.2, Epsilon = 0.95\n",
            "Episode 20: Total Reward = -6.0, Epsilon = 0.90\n",
            "Episode 30: Total Reward = -10.899999999999995, Epsilon = 0.86\n",
            "Episode 40: Total Reward = -6.7, Epsilon = 0.81\n",
            "Episode 50: Total Reward = -6.3, Epsilon = 0.77\n",
            "Episode 60: Total Reward = -6.4, Epsilon = 0.74\n",
            "Episode 70: Total Reward = -5.4, Epsilon = 0.70\n",
            "Episode 80: Total Reward = -6.2, Epsilon = 0.67\n",
            "Episode 90: Total Reward = -9.99999999999998, Epsilon = 0.63\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pygame\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class GridworldEnv:\n",
        "    def __init__(self, grid_size=(10, 10)):\n",
        "        self.grid_size = grid_size\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [grid_size[0] - 1, grid_size[1] - 1]\n",
        "        self.obstacles = [[1, 1], [1, 2], [2, 1], [2, 2], [5,5], [5,6], [6,5], [6,6]]\n",
        "        self.state = self._get_state()\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.state = self._get_state()\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # Up\n",
        "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
        "        elif action == 1:  # Down\n",
        "            self.agent_pos[0] = min(self.grid_size[0] - 1, self.agent_pos[0] + 1)\n",
        "        elif action == 2:  # Left\n",
        "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
        "        elif action == 3:  # Right\n",
        "            self.agent_pos[1] = min(self.grid_size[1] - 1, self.agent_pos[1] + 1)\n",
        "\n",
        "        self.state = self._get_state()\n",
        "        done = False\n",
        "        reward = -0.1  # Small negative reward for each step\n",
        "\n",
        "        if self.agent_pos == self.goal_pos:\n",
        "            reward = 10.0\n",
        "            done = True\n",
        "        elif self.agent_pos in self.obstacles:\n",
        "            reward = -5.0\n",
        "            done = True\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def render(self, screen, cell_size):\n",
        "        screen.fill(WHITE)\n",
        "        for row in range(self.grid_size[0]):\n",
        "            for col in range(self.grid_size[1]):\n",
        "                rect = pygame.Rect(col * cell_size, row * cell_size, cell_size, cell_size)\n",
        "                pygame.draw.rect(screen, BLACK, rect, 1)\n",
        "\n",
        "        # Draw obstacles\n",
        "        for obs in self.obstacles:\n",
        "            rect = pygame.Rect(obs[1] * cell_size, obs[0] * cell_size, cell_size, cell_size)\n",
        "            pygame.draw.rect(screen, RED, rect)\n",
        "\n",
        "        # Draw goal\n",
        "        goal_rect = pygame.Rect(self.goal_pos[1] * cell_size, self.goal_pos[0] * cell_size, cell_size, cell_size)\n",
        "        pygame.draw.rect(screen, GREEN, goal_rect)\n",
        "\n",
        "        # Draw agent\n",
        "        agent_rect = pygame.Rect(self.agent_pos[1] * cell_size, self.agent_pos[0] * cell_size, cell_size, cell_size)\n",
        "        pygame.draw.rect(screen, BLUE, agent_rect)\n",
        "\n",
        "        pygame.display.flip()\n",
        "\n",
        "\n",
        "    def _get_state(self):\n",
        "        state = np.zeros(self.grid_size)\n",
        "        state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
        "        return state\n",
        "\n",
        "def create_dqn_model(input_shape, num_actions):\n",
        "    \"\"\"\n",
        "    Creates a Deep Q-Network (DQN) model.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): The shape of the input state.\n",
        "        num_actions (int): The number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.models.Model: The compiled DQN model.\n",
        "    \"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu')(conv1)\n",
        "\n",
        "    # Flatten the output\n",
        "    flatten = Flatten()(conv2)\n",
        "\n",
        "    # Dense layers\n",
        "    dense1 = Dense(128, activation='relu')(flatten)\n",
        "    output_layer = Dense(num_actions, activation='linear')(dense1)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize pygame\n",
        "pygame.init()\n",
        "\n",
        "# Set up display dimensions\n",
        "env = GridworldEnv()\n",
        "GRID_SIZE = env.grid_size\n",
        "CELL_SIZE = 50\n",
        "DISPLAY_WIDTH = GRID_SIZE[1] * CELL_SIZE\n",
        "DISPLAY_HEIGHT = GRID_SIZE[0] * CELL_SIZE\n",
        "screen = pygame.display.set_mode((DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
        "pygame.display.set_caption(\"Gridworld DQN Training\")\n",
        "\n",
        "# Define colors\n",
        "WHITE = (255, 255, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "BLUE = (0, 0, 255)  # Agent\n",
        "GREEN = (0, 255, 0) # Goal\n",
        "RED = (255, 0, 0) # Obstacle (using RED for obstacles)\n",
        "\n",
        "# Hyperparameters\n",
        "num_episodes = 100\n",
        "max_steps_per_episode = 100\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay_rate = 0.995\n",
        "min_epsilon = 0.01\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99  # Discount factor\n",
        "replay_buffer_size = 10000\n",
        "batch_size = 32\n",
        "target_update_frequency = 10\n",
        "\n",
        "# Instantiate the environment and the main DQN model\n",
        "state_shape = (env.grid_size[0], env.grid_size[1], 1) # Add channel dimension\n",
        "num_actions = 4 # Up, Down, Left, Right\n",
        "main_dqn = create_dqn_model(state_shape, num_actions)\n",
        "\n",
        "# Create the target network and copy initial weights\n",
        "target_dqn = create_dqn_model(state_shape, num_actions)\n",
        "target_dqn.set_weights(main_dqn.get_weights())\n",
        "\n",
        "# Experience Replay Buffer\n",
        "replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.expand_dims(state, axis=-1) # Add channel dimension\n",
        "    state = np.expand_dims(state, axis=0) # Add batch dimension\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randrange(num_actions)\n",
        "        else:\n",
        "            q_values = main_dqn.predict(state, verbose=0)\n",
        "            action = np.argmax(q_values[0])\n",
        "\n",
        "        # Take the selected action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.expand_dims(next_state, axis=-1) # Add channel dimension\n",
        "        next_state = np.expand_dims(next_state, axis=0) # Add batch dimension\n",
        "        total_reward += reward\n",
        "\n",
        "        # Store experience in replay buffer\n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Render the environment\n",
        "        env.render(screen, CELL_SIZE)\n",
        "        pygame.time.wait(100) # Add a small delay for visualization\n",
        "\n",
        "        # Train the main DQN model if buffer is large enough\n",
        "        if len(replay_buffer) > batch_size:\n",
        "            minibatch = random.sample(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "            states = np.vstack(states)\n",
        "            next_states = np.vstack(next_states)\n",
        "\n",
        "            target_q_values = target_dqn.predict(next_states, verbose=0)\n",
        "            # Use argmax on the main network's predictions for the next state\n",
        "            main_q_values_next = main_dqn.predict(next_states, verbose=0)\n",
        "            next_actions = np.argmax(main_q_values_next, axis=1)\n",
        "\n",
        "            # Bellman equation\n",
        "            target_q = rewards + gamma * np.max(target_q_values, axis=1) * (1 - np.array(dones))\n",
        "\n",
        "            # Create target Q-values for the main network\n",
        "            with tf.GradientTape() as tape:\n",
        "                current_q_values = main_dqn(states)\n",
        "                one_hot_actions = tf.one_hot(actions, num_actions)\n",
        "                predicted_q_values = tf.reduce_sum(current_q_values * one_hot_actions, axis=1)\n",
        "                loss = tf.keras.losses.MSE(target_q, predicted_q_values)\n",
        "\n",
        "            grads = tape.gradient(loss, main_dqn.trainable_variables)\n",
        "            main_dqn.optimizer.apply_gradients(zip(grads, main_dqn.trainable_variables))\n",
        "\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)\n",
        "\n",
        "    # Update target network\n",
        "    if episode % target_update_frequency == 0:\n",
        "        target_dqn.set_weights(main_dqn.get_weights())\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}: Total Reward = {total_reward}, Epsilon = {epsilon:.2f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "pygame.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5911dad1"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "\n",
        "# Initialize pygame\n",
        "pygame.init()\n",
        "\n",
        "# Set up display dimensions\n",
        "GRID_SIZE = env.grid_size\n",
        "CELL_SIZE = 50\n",
        "DISPLAY_WIDTH = GRID_SIZE[1] * CELL_SIZE\n",
        "DISPLAY_HEIGHT = GRID_SIZE[0] * CELL_SIZE\n",
        "screen = pygame.display.set_mode((DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
        "pygame.display.set_caption(\"Gridworld DQN Training\")\n",
        "\n",
        "# Define colors\n",
        "WHITE = (255, 255, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "BLUE = (0, 0, 255)  # Agent\n",
        "GREEN = (0, 255, 0) # Goal\n",
        "RED = (255, 0, 0) # Obstacle (using RED for obstacles)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNGbZeE9xYQg+iClxDTQeJX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}