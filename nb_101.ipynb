{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisar/ai_notebooks/blob/main/nb_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POq3h5FtZLzh"
      },
      "outputs": [],
      "source": [
        "# Authentication and service account setup\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import os\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set project ID\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'mrc-quant-ml'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1-i_rat0rMb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q google-genai google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize\n",
        "from google.genai import Client\n",
        "from google.genai.types import Part, VideoMetadata, FileData\n",
        "from google.cloud import storage\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "# import moviepy.editor as mp # You might need to install moviepy\n",
        "import nest_asyncio\n",
        "\n",
        "\n",
        "def summarize_video_chunk(video_uri: str, start_offset: str, end_offset: str, prompt: str = \"Analyze this video and provide a summary.\"):\n",
        "    \"\"\"Summarizes a video chunk using the Gemini API.\"\"\"\n",
        "    client = Client(\n",
        "        vertexai=True,\n",
        "        project=\"mrc-quant-ml\",\n",
        "        location=\"us-central1\",\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        contents=[\n",
        "            Part(\n",
        "                video_metadata=VideoMetadata(\n",
        "                    fps=1,\n",
        "                    start_offset=start_offset,\n",
        "                    end_offset=end_offset\n",
        "                ),\n",
        "                file_data=FileData(\n",
        "                    file_uri=video_uri,\n",
        "                    mime_type=\"video/mp4\",\n",
        "                ),\n",
        "            ),\n",
        "            prompt\n",
        "        ],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Function to get video duration\n",
        "async def get_video_duration(video_uri: str) -> int:\n",
        "    \"\"\"Gets the duration of a video from a GCS URI.\"\"\"\n",
        "    try:\n",
        "        # Assuming the video is in a GCS bucket\n",
        "        client = storage.Client()\n",
        "        bucket_name, blob_name = video_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
        "        bucket = client.get_bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        # Download the video temporarily to get duration (consider optimizing this)\n",
        "        temp_file = f\"/tmp/{blob_name.split('/')[-1]}\"\n",
        "        blob.download_to_filename(temp_file)\n",
        "        clip = mp.VideoFileClip(temp_file)\n",
        "        duration = int(clip.duration)\n",
        "        os.remove(temp_file) # Clean up the temporary file\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting video duration: {e}\")\n",
        "        return 0 # Return 0 or raise an error based on desired behavior\n",
        "\n",
        "# Batch processing optimization\n",
        "async def process_video_chunks_parallel(video_uri: str, chunk_duration_minutes: int = 30, max_workers: int = 4):\n",
        "    \"\"\"Process video chunks in parallel for better throughput with 30-minute intervals.\"\"\"\n",
        "    chunk_duration = chunk_duration_minutes * 60 # Convert minutes to seconds\n",
        "\n",
        "    # Get video duration first\n",
        "    total_duration = await get_video_duration(video_uri)\n",
        "    print(f\"Total video duration: {total_duration} seconds\")\n",
        "    if total_duration == 0:\n",
        "        print(\"Could not get video duration. Aborting processing.\")\n",
        "        return []\n",
        "\n",
        "    chunks = [(i, min(i + chunk_duration, total_duration))\n",
        "              for i in range(0, total_duration, chunk_duration)]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        tasks = [\n",
        "            loop.run_in_executor(\n",
        "                executor,\n",
        "                summarize_video_chunk,\n",
        "                video_uri,\n",
        "                f\"{start}s\",\n",
        "                f\"{end}s\"\n",
        "            ) for start, end in chunks\n",
        "        ]\n",
        "\n",
        "        summaries = await asyncio.gather(*tasks)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Example usage with error handling\n",
        "video_uri = \"gs://mrc-quant-ml-video-analysis/videoplayback.mp4\"\n",
        "\n",
        "# Example of how to use the parallel processing function\n",
        "\n",
        "nest_asyncio.apply() # Apply this if running in Colab\n",
        "\n",
        "try:\n",
        "    all_summaries = asyncio.run(process_video_chunks_parallel(video_uri, chunk_duration_minutes=30))\n",
        "    for i, summary in enumerate(all_summaries):\n",
        "        print(f\"Summary for chunk {i+1}:\\n{summary}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during parallel processing: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IczU0hq4v1G7",
        "outputId": "b19f577b-9e52-4e99-9977-6b14ae155dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total video duration: 7302 seconds\n",
            "Error during parallel processing: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B05l4kUcxYF5",
        "outputId": "7f9c8ffd-6e80-4353-877e-568220bbfa96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here is a summary of the video:\\n\\nThe creator starts the video by performing some audio testing on YouTube and X (formerly Twitter).  After, he transitions into a discussion on a scientific paper, \"Diffusion Beats Autoregressive in Data-Constrained Settings.\" The thumbnail was generated using an AI image generator, although it misspells the word \"autoregressive\" with three s\\'s.\\n\\nHe then reviews the main points of the paper. Diffusion models have had better results due to their ability to reuse data. The creator references a video on YouTube that further discusses how the noise that is added to training images causes them to become diffused. However, the results from the study are not definitive since they may be the result of models overfitting on specific niches. The creator also looks into compute power. Finally, he touches on the \"Chinchilla scaling laws\" which attempt to compute the optimal model.',\n",
              " 'Here is a summary of the video in question.\\n\\nThe speaker in this video is providing his thoughts on the topic of Diffusion models as he navigates through several research papers on his laptop screen. One of the papers he references is \"Distillation Scaling Laws.\" He explains to the audience that the main chart within the publication includes some lines that are solid and some that are dotted. The solid lines represent Interpolation while the dotted lines represent Extrapolation. He argues that the data which the authors provide via interpolation is more accurate to the data than what is provided via extrapolation. He believes it is incorrect to call the \"Distillation Scaling Laws\" since such relationships are better described as intuitions, rather than hard laws. He then briefly transitions over to images of noise and argues that there is a great data argumentation within the models and that diffusion models can therefore yield better performance.',\n",
              " 'Here is a summary of the video:\\n\\nThe video presenter discusses the differences between diffusion-based language models and autoregressive language models, and how each model has different benefits in regard to training with small data sets. \\n\\nThe presenter mentions that Autoregressive language models tend to perform well when the model is tasked with language-based reasoning.\\n\\nA diffusion model, however, can benefit models from more training, whereas autoregressive models begin to overfit as the number of times that the training is performed increases. \\n\\nThe presenter argues that the use of diffusion, as opposed to Autoregressive language models is a better strategy because it results in \"less bias.\"',\n",
              " 'Here is a summary of the video:\\n\\nThe video is about a man discussing the benefits of using a variety of data models. He then gives his opinion that \"the more AI continues to improve...  you lose your humanity... that is why you need to take a break.\" Later, he references a model that allows you to explore \"20,000 GPU hours.\" The conversation then transitions to a more general discussion of AI as it relates to the human experience.',\n",
              " \"Okay, here's a summary of the video:\\n\\nThe video shows a person reviewing research papers related to AI and machine learning, specifically focusing on scaling laws for models and the comparison between diffusion and autoregressive models. \\n\\nThey advise viewers that while scientific studies provide valuable intuitions, it is important not to extrapolate too much beyond the observed data, as the observed trends may not hold true under different conditions. They say that interpolation is safe, but extrapolation is not.\\n\\nThe streamer thanks people in the chat by name, wishes people a good weekend, and recommends that people go out and train a diffusion model.\\n\\nAt the end of the video, the streamer makes a loud noise with a gong.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sw89tIpWxuG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnJAcaBcCNE51ceF1A/TQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}