{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1P4utH8b2WQByNqm3ScXw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisar/ai_notebooks/blob/main/nb_104.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PepMVlV05z4k"
      },
      "outputs": [],
      "source": [
        "# Authentication and service account setup\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import os\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set project ID\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'mrc-quant-ml'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q google-genai google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "PYuH2rC-7HlG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize\n",
        "from google.genai import Client\n",
        "from google.genai.types import Part, VideoMetadata, FileData\n",
        "from google.cloud import storage\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import nest_asyncio\n",
        "import time # Import time module for delays\n",
        "import moviepy.editor as mp # Import moviepy for video duration\n",
        "\n",
        "\n",
        "def summarize_video_chunk(video_uri: str, start_offset: str, end_offset: str, prompt: str = \"Analyze this video and provide a summary.\"):\n",
        "    \"\"\"Summarizes a video chunk using the Gemini API.\"\"\"\n",
        "    client = Client(\n",
        "        vertexai=True,\n",
        "        project=\"mrc-quant-ml\",\n",
        "        location=\"us-central1\",\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        contents=[\n",
        "            Part(\n",
        "                video_metadata=VideoMetadata(\n",
        "                    fps=1,\n",
        "                    start_offset=start_offset,\n",
        "                    end_offset=end_offset\n",
        "                ),\n",
        "                file_data=FileData(\n",
        "                    file_uri=video_uri,\n",
        "                    mime_type=\"video/mp4\",\n",
        "                ),\n",
        "            ),\n",
        "            prompt\n",
        "        ],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Function to get video duration\n",
        "async def get_video_duration(video_uri: str) -> int:\n",
        "    \"\"\"Gets the duration of a video from a GCS URI.\"\"\"\n",
        "    try:\n",
        "        # Assuming the video is in a GCS bucket\n",
        "        client = storage.Client()\n",
        "        bucket_name, blob_name = video_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
        "        bucket = client.get_bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        # Download the video temporarily to get duration (consider optimizing this)\n",
        "        temp_file = f\"/tmp/{blob_name.split('/')[-1]}\"\n",
        "        blob.download_to_filename(temp_file)\n",
        "        clip = mp.VideoFileClip(temp_file)\n",
        "        duration = int(clip.duration)\n",
        "        os.remove(temp_file) # Clean up the temporary file\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting video duration: {e}\")\n",
        "        return 0 # Return 0 or raise an error based on desired behavior\n",
        "\n",
        "# Batch processing optimization\n",
        "async def process_video_chunks_parallel(video_uri: str, chunk_duration_minutes: int = 30, max_workers: int = 4, delay_seconds: int = 1):\n",
        "    \"\"\"Process video chunks in parallel for better throughput with 30-minute intervals and a delay between API calls.\"\"\"\n",
        "    chunk_duration = chunk_duration_minutes * 60 # Convert minutes to seconds\n",
        "\n",
        "    total_duration = 7302 # await get_video_duration(video_uri)\n",
        "    print(f\"Total video duration: {total_duration} seconds\")\n",
        "    if total_duration == 0:\n",
        "        print(\"Could not get video duration. Aborting processing.\")\n",
        "        return []\n",
        "\n",
        "    chunks = [(i, min(i + chunk_duration, total_duration))\n",
        "              for i in range(0, total_duration, chunk_duration)]\n",
        "\n",
        "    def run_summarize_chunk(start, end):\n",
        "      \"\"\"Helper function to run the summarize_video_chunk coroutine.\"\"\"\n",
        "      return asyncio.run(process_chunk_with_delay(start, end))\n",
        "\n",
        "    async def process_chunk_with_delay(start, end):\n",
        "        \"\"\"Helper function to process a chunk with a delay.\"\"\"\n",
        "        summary = summarize_video_chunk(video_uri, f\"{start}s\", f\"{end}s\")\n",
        "        await asyncio.sleep(delay_seconds) # Add delay between calls\n",
        "        return summary\n",
        "\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        tasks = [\n",
        "            loop.run_in_executor(\n",
        "                executor,\n",
        "                run_summarize_chunk,\n",
        "                start,\n",
        "                end\n",
        "            ) for start, end in chunks\n",
        "        ]\n",
        "\n",
        "        summaries = await asyncio.gather(*tasks)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Example usage with error handling\n",
        "video_uri = \"gs://mrc-quant-ml-video-analysis/videoplayback.mp4\"\n",
        "\n",
        "# Example of how to use the parallel processing function\n",
        "\n",
        "nest_asyncio.apply() # Apply this if running in Colab\n",
        "\n",
        "try:\n",
        "    # Add delay_seconds parameter to control delay\n",
        "    all_summaries = asyncio.run(process_video_chunks_parallel(video_uri, chunk_duration_minutes=30, delay_seconds=5))\n",
        "    for i, summary in enumerate(all_summaries):\n",
        "        print(f\"Summary for chunk {i+1}:\\n{summary}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during parallel processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PPJ7D6u7Ks6",
        "outputId": "0c766701-d20c-4460-c12e-47de5c44e3ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total video duration: 7302 seconds\n",
            "Summary for chunk 1:\n",
            "Alright, here's a summary of the video:\n",
            "\n",
            "The speaker, Hu-Po, starts by testing live streaming on Youtube and X and then introduces the topic for the live stream: a discussion on the research paper \"Diffusion Beats Autoregressive in Data-Constrained Settings.\"\n",
            "In the live stream, he first explains the origin of the thumbnail and that the word Autoregressive was misspelled. Then he briefly describes the study of masking, the purpose of the said research paper, what makes diffusion models so special, and explains the importance of each item as well as what their purposes are when they are limited by things such as low processing power and low data availability.\n",
            "\n",
            "Summary for chunk 2:\n",
            "The speaker describes how Scaling Laws are sometimes extrapolated beyond the data available. \n",
            "\n",
            "\n",
            "He then provides a brief overview of the concept of attention masks in the context of autoaggressive models. \n",
            "\n",
            "\n",
            "He finally goes on to describe how diffusion models have implicit data augmentation due to being able to produce data from any arbitrary start point which looks like pure noise.\n",
            "\n",
            "Summary for chunk 3:\n",
            "Here is a summary of the video analysis:\n",
            "\n",
            "The video analyst describes diffusion models with a cat image, then brings up two research papers to describe autoregressive language models. The speaker then relates the research and theory into practical implications of AI, like OpenAI and Nvidia. He also touches on the concept of compute and where certain companies are succeeding.\n",
            "\n",
            "Summary for chunk 4:\n",
            "The video discusses the implications of diffusion models on the landscape of AI research. It highlights that AI progress is limited by human cognitive capacity, thereby driving a potential divergence within humanity.\n",
            "The speaker considers a scenario wherein companies that take advantage of AI may not resemble what they are in their current form. Highlighting the limited quantity of quality data, it seems inevitable that researchers will need to rely more heavily on data augmentation.\n",
            "\n",
            "Summary for chunk 5:\n",
            "Here is a summary of the video:\n",
            "\n",
            "At [120:00], the speaker discusses scientific studies and explains that they should be viewed as pieces of intuition. He states that the data points should be used for interpolation but not extrapolation. He then acknowledges viewers of the broadcast and concludes.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f1258ce",
        "outputId": "7f11416d-cd5d-4611-d797-0a684c20c722"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Add Vertex AI imports\n",
        "from google.cloud.aiplatform import init\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "\n",
        "def score_relevancy_gemini(user_query: str, summaries: list[str], model_name: str = \"text-embedding-004\") -> list[float]:\n",
        "    \"\"\"\n",
        "    Calculates relevancy scores for a list of summaries based on a user query\n",
        "    using Gemini embedding and cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        user_query: The user's query string.\n",
        "        summaries: A list of summary strings.\n",
        "        model_name: The name of the Gemini embedding model to use (default is text-embedding-004).\n",
        "\n",
        "    Returns:\n",
        "        A list of cosine similarity scores, one for each summary.\n",
        "    \"\"\"\n",
        "    # Initialize Vertex AI (if not already initialized)\n",
        "    init(project=\"mrc-quant-ml\", location=\"us-central1\")\n",
        "\n",
        "    # Get the embedding model\n",
        "    embedding_model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "\n",
        "    # Embed the user query and each summary using Vertex AI\n",
        "    query_embedding = embedding_model.get_embeddings([user_query])[0].values\n",
        "    summary_embeddings = [embedding.values for embedding in embedding_model.get_embeddings(summaries)]\n",
        "\n",
        "    # Calculate the cosine similarity between the query embedding and each summary embedding\n",
        "    # cosine_similarity expects a 2D array for the first argument, so reshape the query embedding\n",
        "    similarity_scores = cosine_similarity([query_embedding], summary_embeddings)\n",
        "\n",
        "    # The result of cosine_similarity is a 2D array, so flatten it to get a list of scores\n",
        "    return similarity_scores[0].tolist()\n",
        "\n",
        "# Example usage\n",
        "user_query = \"diffusion models\"\n",
        "summaries = all_summaries # Assuming all_summaries is available from previous execution\n",
        "relevancy_scores_gemini = score_relevancy_gemini(user_query, summaries)\n",
        "print(f\"Relevancy scores using Gemini embedding for query '{user_query}': {relevancy_scores_gemini}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevancy scores using Gemini embedding for query 'diffusion models': [0.565487016765596, 0.5798447897620905, 0.5898328088135344, 0.5833395964088023, 0.3505766592289541]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etJ_S_nv9F8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}